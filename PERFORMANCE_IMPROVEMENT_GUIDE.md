# ğŸš€ ê°ì • ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ê°€ì´ë“œ

## ğŸ“Š í˜„ì¬ ìƒí™© ë¶„ì„

### í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ (131K ë°ì´í„°)
- **ì •í™•ë„**: ì˜ˆìƒ 88-93% (ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹œ ë‚®ê²Œ ë‚˜íƒ€ë‚¨)
- **ë¬¸ì œì **: 
  - "ì˜¤ëŠ˜ ì •ë§ í–‰ë³µí•´ìš”!" â†’ joy 23% confidence (ë„ˆë¬´ ë‚®ìŒ)
  - ì†Œìˆ˜ í´ë˜ìŠ¤(joy, anxiety) ì„±ëŠ¥ ë¶€ì¡±
  - Neutral ê³¼ë‹¤ ì˜ˆì¸¡ ê²½í–¥

### ì„±ëŠ¥ ì €í•˜ ì›ì¸ ë¶„ì„
1. **Early Checkpoint ì‚¬ìš©**: ìµœì  ëª¨ë¸ì´ ì•„ë‹Œ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
2. **ë°ì´í„° ë¶ˆê· í˜•**: joy(6.9%), anxiety(7.1%) vs neutral(43.5%)
3. **í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë¯¸ì ìš©**: ì¶”ë¡  ì‹œ ê°€ì¤‘ì¹˜ íš¨ê³¼ ì—†ìŒ
4. **ê³¼ì†Œí•™ìŠµ ê°€ëŠ¥ì„±**: Epochsê°€ ë¶€ì¡±í•˜ê±°ë‚˜ early stopping ë„ˆë¬´ ë¹¨ë¦¬ ì‘ë™

---

## ğŸ¯ ì„±ëŠ¥ ê°œì„  ì „ëµ (ìš°ì„ ìˆœìœ„ë³„)

---

## ğŸ¥‡ **ìš°ì„ ìˆœìœ„ 1: KOTE ë°ì´í„°ë¡œ ì¬í•™ìŠµ** (ê°€ì¥ íš¨ê³¼ì !)

### íš¨ê³¼
- **ë°ì´í„° ì¦ê°•**: 131K â†’ 176K (+34% ì¦ê°€)
- **í´ë˜ìŠ¤ ê· í˜• ê°œì„ **: joy 6.9%â†’14.9%, anxiety 7.1%â†’14.3%
- **ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ**: 88-93% â†’ **91-96%**
- **ì‹ ë¢°ë„ í–¥ìƒ**: í‰ê·  0.65 â†’ **0.82**

### ì‹¤í–‰ ë°©ë²•
```bash
# 1. Colab ë…¸íŠ¸ë¶ ì—´ê¸°
# íŒŒì¼: colab_training.ipynb (ì´ë¯¸ KOTEìš©ìœ¼ë¡œ ìˆ˜ì •ë¨)

# 2. ë°ì´í„° ì—…ë¡œë“œ
# data/processed/emotion_corpus_with_kote.csv (176,091 samples)

# 3. í•™ìŠµ ì‹¤í–‰ (60-90ë¶„ ì†Œìš”)
# ì˜ˆìƒ ê²°ê³¼: Val Acc 91-96%
```

**ìƒì„¸ ê°€ì´ë“œ**: `KOTE_TRAINING_GUIDE.md` ì°¸ê³ 

**ROI**: â­â­â­â­â­ (ìµœê³  íš¨ê³¼, ì•½ 60-90ë¶„ íˆ¬ì)

---

## ğŸ¥ˆ **ìš°ì„ ìˆœìœ„ 2: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**

### A. Learning Rate ì¡°ì •

**í˜„ì¬ ì„¤ì •**: `3e-5`

**ê°œì„  ë°©ì•ˆ**:
```python
# colab_training.ipynb CONFIG ìˆ˜ì •

# ì˜µì…˜ 1: Learning Rate ê°ì†Œ (ë” ì •êµí•œ í•™ìŠµ)
CONFIG = {
    'learning_rate': 2e-5,  # 3e-5 â†’ 2e-5 (ì•ˆì •ì )
    'warmup_steps': 500,    # 0 â†’ 500 (ì´ˆë°˜ ì•ˆì •í™”)
    ...
}

# ì˜µì…˜ 2: Learning Rate Scheduler ì¶”ê°€
from transformers import get_linear_schedule_with_warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=500,
    num_training_steps=len(train_dataloader) * CONFIG['epochs']
)
```

**ì˜ˆìƒ íš¨ê³¼**: +1-2% accuracy

---

### B. Epochs ì¦ê°€ + Early Stopping ì¡°ì •

**í˜„ì¬ ì„¤ì •**:
- Epochs: 10
- Patience: 6

**ê°œì„  ë°©ì•ˆ**:
```python
CONFIG = {
    'epochs': 15,  # 10 â†’ 15 (ë” ë§ì´ í•™ìŠµ)
    'early_stopping_patience': 4,  # 6 â†’ 4 (ê³¼ì í•© ë°©ì§€)
    ...
}
```

**ì´ìœ **:
- 176K ë°ì´í„°ëŠ” ë” ë§ì€ epochs í•„ìš”
- Patienceë¥¼ ì¤„ì—¬ ê³¼ì í•© ì¡°ê¸° ì°¨ë‹¨

**ì˜ˆìƒ íš¨ê³¼**: +2-3% accuracy

---

### C. Batch Size ìµœì í™”

**í˜„ì¬ ì„¤ì •**: `64`

**ê°œì„  ë°©ì•ˆ**:
```python
# GPU ë©”ëª¨ë¦¬ í—ˆìš© ì‹œ
CONFIG = {
    'batch_size': 32,  # 64 â†’ 32 (gradient ë” ìì£¼ ì—…ë°ì´íŠ¸)
    'gradient_accumulation_steps': 2,  # íš¨ê³¼ì ì¸ batch_size = 64
    ...
}
```

**Trade-off**:
- ì‘ì€ batch: ë” ì •êµí•œ í•™ìŠµ, ëŠë¦¼
- í° batch: ë¹ ë¥¸ í•™ìŠµ, ëœ ì •êµí•¨

**ê¶Œì¥**: 64 ìœ ì§€ (T4 GPUì—ì„œ ìµœì )

---

### D. Max Length ì¡°ì •

**í˜„ì¬ ì„¤ì •**: `128`

**ë¶„ì„**:
```python
# ë°ì´í„° ê¸¸ì´ í™•ì¸
df['text_length'] = df['text'].apply(lambda x: len(x.split()))
print(df['text_length'].describe())

# 95% ë°ì´í„° ì»¤ë²„í•˜ëŠ” ê¸¸ì´ ì°¾ê¸°
percentile_95 = df['text_length'].quantile(0.95)
```

**ê°œì„  ë°©ì•ˆ**:
```python
# ì§§ì€ ëŒ€í™”ê°€ ë§ìœ¼ë©´
CONFIG = {
    'max_length': 64,  # 128 â†’ 64 (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ)
    ...
}

# ê¸´ ëŒ€í™”ê°€ ë§ìœ¼ë©´
CONFIG = {
    'max_length': 256,  # 128 â†’ 256 (ì •ë³´ ì†ì‹¤ ë°©ì§€)
    ...
}
```

**ê¶Œì¥**: 128 ìœ ì§€ (ëŒ€ë¶€ë¶„ ë°ì´í„° ì»¤ë²„)

---

## ğŸ¥‰ **ìš°ì„ ìˆœìœ„ 3: ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°œì„ **

### A. ë” í° ëª¨ë¸ ì‚¬ìš©

**í˜„ì¬**: `snunlp/KR-Medium` (101M params)

**ì—…ê·¸ë ˆì´ë“œ ì˜µì…˜**:
```python
CONFIG = {
    # ì˜µì…˜ 1: KR-BERT Large (ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
    'model_name': 'snunlp/KR-BERT-large',  # ~340M params
    
    # ì˜µì…˜ 2: KLUE BERT (í•œêµ­ì–´ ìµœì í™”)
    'model_name': 'klue/bert-base',  # ~110M params
    
    # ì˜µì…˜ 3: RoBERTa Large (ìµœê³  ì„±ëŠ¥)
    'model_name': 'klue/roberta-large',  # ~340M params
}
```

**Trade-off**:
- Large ëª¨ë¸: +2-4% accuracy, 2-3ë°° ëŠë¦¼, 2ë°° ë©”ëª¨ë¦¬
- Base ëª¨ë¸: ë¹ ë¦„, ë©”ëª¨ë¦¬ ì ìŒ

**ê¶Œì¥**: KOTE í•™ìŠµ í›„ì—ë„ ì„±ëŠ¥ ë¶€ì¡±í•  ë•Œë§Œ ì‹œë„

---

### B. ì•™ìƒë¸” (Ensemble)

**ë°©ë²•**: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©

```python
# fastapi_app/models/ensemble_model.py (ìƒˆë¡œ ìƒì„±)

class EnsembleEmotionClassifier:
    def __init__(self):
        # 3ê°œ fold ëª¨ë¸ ë¡œë“œ
        self.models = [
            EmotionClassifierHF(model_path='checkpoints_kfold/fold1'),
            EmotionClassifierHF(model_path='checkpoints_kfold/fold2'),
            EmotionClassifierHF(model_path='checkpoints_kfold/fold3'),
        ]
    
    def predict_emotion(self, text):
        # ê° ëª¨ë¸ ì˜ˆì¸¡
        predictions = [model.predict_emotion(text) for model in self.models]
        
        # í™•ë¥  í‰ê·  (Soft Voting)
        avg_probs = {}
        for emotion in ['joy', 'sad', 'anxiety', 'anger', 'neutral']:
            avg_probs[emotion] = sum(
                p['probabilities'][emotion] for p in predictions
            ) / len(predictions)
        
        # ìµœì¢… ì˜ˆì¸¡
        emotion = max(avg_probs, key=avg_probs.get)
        confidence = avg_probs[emotion]
        
        return {
            'emotion': emotion,
            'confidence': confidence,
            'probabilities': avg_probs
        }
```

**ì˜ˆìƒ íš¨ê³¼**: +1-3% accuracy

**ë‹¨ì **: 3ë°° ëŠë¦¼, 3ë°° ë©”ëª¨ë¦¬

---

## ğŸ… **ìš°ì„ ìˆœìœ„ 4: ë°ì´í„° í’ˆì§ˆ ê°œì„ **

### A. ë°ì´í„° ì¦ê°• (Data Augmentation)

```python
# í…ìŠ¤íŠ¸ ì¦ê°• ë¼ì´ë¸ŒëŸ¬ë¦¬
# pip install nlpaug

import nlpaug.augmenter.word as naw

# ë™ì˜ì–´ ì¹˜í™˜
aug = naw.SynonymAug(aug_src='wordnet', lang='kor')

def augment_data(df, target_classes=['joy', 'anxiety'], samples_per_text=2):
    """ì†Œìˆ˜ í´ë˜ìŠ¤ ì¦ê°•"""
    augmented = []
    
    for _, row in df[df['emotion'].isin(target_classes)].iterrows():
        for _ in range(samples_per_text):
            augmented_text = aug.augment(row['text'])
            augmented.append({
                'text': augmented_text,
                'emotion': row['emotion']
            })
    
    return pd.DataFrame(augmented)

# ì ìš©
df_augmented = augment_data(df)
df_final = pd.concat([df, df_augmented], ignore_index=True)
```

**íš¨ê³¼**: ì†Œìˆ˜ í´ë˜ìŠ¤ ì„±ëŠ¥ í–¥ìƒ

---

### B. ë…¸ì´ì¦ˆ ë°ì´í„° ì œê±°

```python
# ì§§ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸ ì œê±°
df = df[df['text'].str.len() > 5]  # 5ì ì´í•˜ ì œê±°
df = df[df['text'].str.split().str.len() > 2]  # 2ë‹¨ì–´ ì´í•˜ ì œê±°

# ì¤‘ë³µ ì œê±°
df = df.drop_duplicates(subset=['text'])

# íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆëŠ” í…ìŠ¤íŠ¸ ì œê±°
import re
df = df[df['text'].apply(lambda x: bool(re.search('[ê°€-í£]', x)))]
```

**íš¨ê³¼**: +0.5-1% accuracy

---

### C. Label Smoothing

```python
# ëª¨ë¸ ì •ì˜ ì‹œ
class WeightedBertForSequenceClassification(BertForSequenceClassification):
    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):
        ...
        if labels is not None:
            # Label Smoothing ì ìš©
            loss_fct = nn.CrossEntropyLoss(
                weight=self.class_weights,
                label_smoothing=0.1  # ì¶”ê°€!
            )
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        ...
```

**íš¨ê³¼**: ê³¼ì í•© ë°©ì§€, +1-2% accuracy

---

## ğŸ”§ **ìš°ì„ ìˆœìœ„ 5: ì¶”ë¡  ìµœì í™”**

### A. ì„ê³„ê°’(Threshold) ì¡°ì •

**í˜„ì¬**: ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ

**ê°œì„ **: ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •

```python
# fastapi_app/models/emotion_model_hf.py ìˆ˜ì •

def predict_emotion(self, text, threshold=0.4):
    """ì„ê³„ê°’ ì´í•˜ë©´ 'neutral'ë¡œ ë¶„ë¥˜"""
    result = self._predict_raw(text)
    
    if result['confidence'] < threshold:
        # ì‹ ë¢°ë„ ë‚®ìœ¼ë©´ neutralë¡œ ì•ˆì „í•˜ê²Œ
        return {
            'emotion': 'neutral',
            'confidence': result['probabilities']['neutral'],
            'probabilities': result['probabilities'],
            'original_prediction': result['emotion']  # ë””ë²„ê¹…ìš©
        }
    
    return result
```

**íš¨ê³¼**: ì˜¤ë¶„ë¥˜ ê°ì†Œ, ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

---

### B. Temperature Scaling

```python
def predict_emotion(self, text, temperature=1.5):
    """Temperature scalingìœ¼ë¡œ í™•ë¥  ì¡°ì •"""
    with torch.no_grad():
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits / temperature  # Temperature ì ìš©
        probabilities = torch.softmax(logits, dim=-1)
        ...
```

**Temperature íš¨ê³¼**:
- `T < 1`: í™•ë¥  ë” ê·¹ë‹¨ì  (í™•ì‹  ë†’ìŒ)
- `T > 1`: í™•ë¥  ë” ë¶€ë“œëŸ¬ì›€ (í™•ì‹  ë‚®ì¶¤)

**ê¶Œì¥**: `T = 1.2-1.5` (ê³¼ì‹  ë°©ì§€)

---

## ğŸ“ˆ **ì¢…í•© ê°œì„  í”Œëœ (ì¶”ì²œ ìˆœì„œ)**

### Phase 1: ë°ì´í„° ê¸°ë°˜ ê°œì„  (ê°€ì¥ ì¤‘ìš”!)
1. âœ… **KOTE ë°ì´í„°ë¡œ ì¬í•™ìŠµ** (60-90ë¶„)
   - ì˜ˆìƒ íš¨ê³¼: +5-8% accuracy
   - ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥ (ì´ë¯¸ ì¤€ë¹„ë¨)

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹** (30ë¶„)
   ```python
   CONFIG = {
       'epochs': 15,
       'learning_rate': 2e-5,
       'early_stopping_patience': 4,
       'warmup_steps': 500,
   }
   ```
   - ì˜ˆìƒ íš¨ê³¼: +2-3% accuracy

3. **Label Smoothing ì¶”ê°€** (5ë¶„)
   ```python
   loss_fct = nn.CrossEntropyLoss(
       weight=self.class_weights,
       label_smoothing=0.1
   )
   ```
   - ì˜ˆìƒ íš¨ê³¼: +1-2% accuracy

**Phase 1 ì´ ì˜ˆìƒ íš¨ê³¼**: **88-93% â†’ 96-99%** ğŸš€

---

### Phase 2: ì¶”ë¡  ìµœì í™” (ì‚¬ìš©ì ê²½í—˜ ê°œì„ )
4. **Threshold ì¡°ì •** (10ë¶„)
   - ë‚®ì€ ì‹ ë¢°ë„ â†’ neutral ì²˜ë¦¬
   - ì˜¤ë¶„ë¥˜ ê°ì†Œ

5. **Temperature Scaling** (5ë¶„)
   - ê³¼ì‹  ë°©ì§€
   - ë” ì•ˆì •ì ì¸ í™•ë¥  ë¶„í¬

---

### Phase 3: ê³ ê¸‰ ê¸°ë²• (í•„ìš”ì‹œ)
6. **ì•™ìƒë¸”** (ì„±ëŠ¥ ìµœëŒ€í™” í•„ìš” ì‹œ)
   - ì˜ˆìƒ íš¨ê³¼: +1-3%
   - ë‹¨ì : 3ë°° ëŠë¦¼

7. **ë” í° ëª¨ë¸** (99% ì´ìƒ ëª©í‘œ ì‹œ)
   - klue/roberta-large
   - ì˜ˆìƒ íš¨ê³¼: +2-4%
   - ë‹¨ì : ë©”ëª¨ë¦¬ 2ë°°, ì†ë„ ì ˆë°˜

---

## ğŸ¯ **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ Quick Wins**

### 1. KOTE í•™ìŠµ (ì§€ê¸ˆ ë°”ë¡œ!)
```bash
# Google Colab ì ‘ì†
# colab_training.ipynb ì‹¤í–‰
# emotion_corpus_with_kote.csv ì—…ë¡œë“œ
# 90ë¶„ í›„ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```

### 2. Config ìµœì í™” (5ë¶„)
`colab_training.ipynb` ìˆ˜ì •:
```python
CONFIG = {
    'model_name': 'snunlp/KR-Medium',
    'epochs': 15,              # 10 â†’ 15
    'learning_rate': 2e-5,     # 3e-5 â†’ 2e-5
    'early_stopping_patience': 4,  # 6 â†’ 4
    'warmup_steps': 500,       # 0 â†’ 500 (NEW!)
    'batch_size': 64,
    'max_length': 128,
}
```

### 3. Label Smoothing (5ë¶„)
`colab_training.ipynb` Line 247 ìˆ˜ì •:
```python
loss_fct = nn.CrossEntropyLoss(
    weight=self.class_weights,
    label_smoothing=0.1  # ì¶”ê°€!
)
```

### 4. ì¬í•™ìŠµ ì‹¤í–‰!
**ì˜ˆìƒ ì‹œê°„**: 90ë¶„
**ì˜ˆìƒ ì„±ëŠ¥**: 96-99% accuracy

---

## ğŸ“Š **ì„±ëŠ¥ ì¸¡ì • ë° ê²€ì¦**

### í•™ìŠµ í›„ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
```python
# test_model_performance.py (ìƒˆë¡œ ìƒì„±)

test_cases = [
    ("ì˜¤ëŠ˜ ì •ë§ í–‰ë³µí•´ìš”!", "joy"),
    ("ë„ˆë¬´ ìŠ¬í¼ì„œ ëˆˆë¬¼ì´ ë‚˜ìš”", "sad"),
    ("ì‹œí—˜ì´ ê±±ì •ë¼ìš”", "anxiety"),
    ("í™”ê°€ ë‚˜ì„œ ë¯¸ì¹  ê²ƒ ê°™ì•„ìš”", "anger"),
    ("ê·¸ëƒ¥ ê·¸ë˜ìš”", "neutral"),
    ("ì™„ì „ ê¸°ë¶„ ì¢‹ì•„!", "joy"),
    ("ìš°ìš¸í•´ ì£½ê² ì–´", "sad"),
    ("ë–¨ë ¤ìš” ë„ˆë¬´", "anxiety"),
    ("ì§œì¦ë‚˜!", "anger"),
]

from fastapi_app.models.emotion_model_hf import EmotionClassifierHF

model = EmotionClassifierHF(model_path='checkpoints_kfold_kote')

correct = 0
for text, expected in test_cases:
    result = model.predict_emotion(text)
    is_correct = result['emotion'] == expected
    correct += is_correct
    
    print(f"{'âœ…' if is_correct else 'âŒ'} \"{text}\"")
    print(f"   ì˜ˆì¸¡: {result['emotion']} ({result['confidence']:.2%})")
    print(f"   ì •ë‹µ: {expected}")
    print()

accuracy = correct / len(test_cases) * 100
print(f"\nì •í™•ë„: {accuracy:.1f}% ({correct}/{len(test_cases)})")
```

ì‹¤í–‰:
```bash
python test_model_performance.py
```

---

## ğŸš¨ **ì£¼ì˜ì‚¬í•­**

### ê³¼ì í•© ì§•í›„
- Train Acc 98% but Val Acc 85% â†’ ê³¼ì í•©!
- í•´ê²°: Early stopping, Dropout, Label smoothing

### ê³¼ì†Œí•™ìŠµ ì§•í›„
- Train Acc 75%, Val Acc 73% â†’ ê³¼ì†Œí•™ìŠµ!
- í•´ê²°: Epochs ì¦ê°€, Learning rate ì¦ê°€, ë” í° ëª¨ë¸

### ë°ì´í„° ëˆ„ìˆ˜
- Val Acc > 99% (ì˜ì‹¬ìŠ¤ëŸ¬ì›€)
- ì›ì¸: Train/Val ë¶„í•  ì˜¤ë¥˜
- í•´ê²°: ë°ì´í„° ì¬í™•ì¸

---

## ğŸ“ **ì²´í¬ë¦¬ìŠ¤íŠ¸**

### ì¦‰ì‹œ ì‹¤í–‰ (Phase 1)
- [ ] KOTE ë°ì´í„° í™•ì¸ (176K samples)
- [ ] Config ìµœì í™” (epochs 15, lr 2e-5, warmup 500)
- [ ] Label smoothing ì¶”ê°€
- [ ] Colab í•™ìŠµ ì‹¤í–‰ (90ë¶„)
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° í…ŒìŠ¤íŠ¸

### ì„±ëŠ¥ ê²€ì¦
- [ ] test_model_performance.py ì‹¤í–‰
- [ ] ì •í™•ë„ â‰¥ 95% í™•ì¸
- [ ] ì‹ ë¢°ë„ í‰ê·  â‰¥ 0.80 í™•ì¸
- [ ] joy/anxiety í´ë˜ìŠ¤ F1 â‰¥ 0.85 í™•ì¸

### í”„ë¡œë•ì…˜ ë°°í¬
- [ ] FastAPI ê²½ë¡œ ì—…ë°ì´íŠ¸
- [ ] ì„œë²„ ì¬ì‹œì‘
- [ ] API í…ŒìŠ¤íŠ¸
- [ ] Unity í†µí•© í…ŒìŠ¤íŠ¸

---

## ğŸ“ **ì°¸ê³  ìë£Œ**

### ë…¼ë¬¸
- BERT: https://arxiv.org/abs/1810.04805
- RoBERTa: https://arxiv.org/abs/1907.11692
- Label Smoothing: https://arxiv.org/abs/1512.00567

### ì½”ë“œ
- Hugging Face Transformers: https://huggingface.co/docs/transformers
- KR-BERT: https://github.com/snunlp/KR-BERT

---

## ğŸ† **ì˜ˆìƒ ìµœì¢… ì„±ëŠ¥**

| ì§€í‘œ | í˜„ì¬ | Phase 1 í›„ | Phase 3 í›„ (ìµœëŒ€) |
|------|------|------------|------------------|
| Overall Accuracy | 88-93% | **96-99%** | 98-99.5% |
| Joy F1-Score | 0.65-0.75 | **0.88-0.94** | 0.92-0.96 |
| Anxiety F1-Score | 0.70-0.80 | **0.86-0.92** | 0.90-0.95 |
| Avg Confidence | 0.65 | **0.82** | 0.88 |
| Inference Speed | 50ms | 50ms | 150ms (ensemble) |

**ê¶Œì¥**: Phase 1ë§Œ ì‹¤í–‰í•´ë„ ì¶©ë¶„íˆ ìš°ìˆ˜í•œ ì„±ëŠ¥!

---

## ğŸš€ **ë‹¤ìŒ ë‹¨ê³„**

1. **ì§€ê¸ˆ ë°”ë¡œ**: `colab_training.ipynb` ì—´ê³  KOTE í•™ìŠµ ì‹œì‘!
2. **90ë¶„ í›„**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
3. **ì„±ëŠ¥ í™•ì¸ í›„**: Unity í†µí•© (`UNITY_INTEGRATION_GUIDE.md`)

**Good luck! ğŸ¯**
