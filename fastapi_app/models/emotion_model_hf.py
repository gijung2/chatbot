"""
ê°ì • ë¶„ë¥˜ ëª¨ë¸ (Hugging Face Transformers ê¸°ë°˜)
í•™ìŠµëœ KR-BERT ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
"""
import torch
from typing import Dict
import logging
from pathlib import Path

from transformers import AutoTokenizer, AutoModelForSequenceClassification

logger = logging.getLogger(__name__)


class EmotionClassifierHF:
    """Hugging Face Transformers ê¸°ë°˜ ê°ì • ë¶„ë¥˜ ëª¨ë¸"""
    
    def __init__(
        self,
        model_path: str = None,
        device: str = 'cpu',
        num_classes: int = 5
    ):
        """
        Args:
            model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (ì˜ˆ: checkpoints_kfold/fold1_best_model_20251104_XXXXXX)
            device: 'cpu' ë˜ëŠ” 'cuda'
            num_classes: ê°ì • í´ë˜ìŠ¤ ìˆ˜ (ê¸°ë³¸: 5)
        """
        self.device = device
        self.num_classes = num_classes
        
        # ê°ì • ë¼ë²¨ (í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ìˆœì„œì™€ ë™ì¼í•˜ê²Œ)
        self.emotion_labels = ["joy", "sad", "anxiety", "anger", "neutral"]
        self.label_to_id = {label: i for i, label in enumerate(self.emotion_labels)}
        self.id_to_label = {i: label for i, label in enumerate(self.emotion_labels)}
        
        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self._load_model(model_path)
        
        logger.info(f"âœ… ê°ì • ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   - ëª¨ë¸ ê²½ë¡œ: {model_path}")
        logger.info(f"   - Device: {device}")
        logger.info(f"   - ê°ì • í´ë˜ìŠ¤: {self.emotion_labels}")
    
    def _load_model(self, model_path: str = None):
        """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        if model_path is None:
            # ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ (ê°€ì¥ ìµœì‹  í•™ìŠµ ëª¨ë¸ ìë™ íƒìƒ‰)
            model_path = self._find_latest_model()
        
        if model_path is None:
            raise ValueError(
                "ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•˜ê±°ë‚˜ checkpoints_kfold/ í´ë”ì— í•™ìŠµëœ ëª¨ë¸ì„ ë°°ì¹˜í•˜ì„¸ìš”."
            )
        
        try:
            logger.info(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _find_latest_model(self) -> str:
        """ìµœì‹  í•™ìŠµ ëª¨ë¸ ìë™ íƒìƒ‰"""
        project_root = Path(__file__).parent.parent.parent
        checkpoints_dir = project_root / "checkpoints_kfold"
        
        if not checkpoints_dir.exists():
            return None
        
        # fold*_best_model_* í˜•ì‹ì˜ í´ë” ì°¾ê¸°
        model_dirs = list(checkpoints_dir.glob("fold*_best_model_*"))
        
        if not model_dirs:
            logger.warning(f"âš ï¸ {checkpoints_dir}ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ê°€ì¥ ìµœì‹  í´ë” ì„ íƒ (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
        latest_model = max(model_dirs, key=lambda p: p.name)
        logger.info(f"ğŸ” ìµœì‹  ëª¨ë¸ ë°œê²¬: {latest_model.name}")
        
        return str(latest_model)
    
    def predict_emotion(
        self,
        text: str,
        max_length: int = 128
    ) -> Dict:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ê°ì • ì˜ˆì¸¡
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            
        Returns:
            {
                'emotion': str,           # ì˜ˆì¸¡ëœ ê°ì • ('joy', 'sad', 'anxiety', 'anger', 'neutral')
                'confidence': float,      # ì‹ ë¢°ë„ (0-1)
                'probabilities': dict     # ê° ê°ì •ë³„ í™•ë¥ 
            }
        """
        self.model.eval()
        
        # í† í°í™”
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            predicted_class = torch.argmax(probabilities, dim=-1).item()
            confidence = probabilities[0][predicted_class].item()
        
        return {
            'emotion': self.emotion_labels[predicted_class],
            'confidence': float(confidence),
            'probabilities': {
                self.emotion_labels[i]: float(prob.item())
                for i, prob in enumerate(probabilities[0])
            }
        }
    
    def predict_batch(
        self,
        texts: list[str],
        max_length: int = 128
    ) -> list[Dict]:
        """ë°°ì¹˜ ì˜ˆì¸¡ (ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë™ì‹œ ì²˜ë¦¬)"""
        self.model.eval()
        
        # ë°°ì¹˜ í† í°í™”
        encoding = self.tokenizer(
            texts,
            add_special_tokens=True,
            max_length=max_length,
            padding=True,
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        # ë°°ì¹˜ ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            predicted_classes = torch.argmax(probabilities, dim=-1)
        
        # ê²°ê³¼ ë³€í™˜
        results = []
        for i in range(len(texts)):
            predicted_class = predicted_classes[i].item()
            confidence = probabilities[i][predicted_class].item()
            
            results.append({
                'emotion': self.emotion_labels[predicted_class],
                'confidence': float(confidence),
                'probabilities': {
                    self.emotion_labels[j]: float(probabilities[i][j].item())
                    for j in range(self.num_classes)
                }
            })
        
        return results
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        return {
            'model_type': type(self.model).__name__,
            'num_classes': self.num_classes,
            'emotion_labels': self.emotion_labels,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'device': str(self.device),
            'tokenizer_vocab_size': self.tokenizer.vocab_size
        }
