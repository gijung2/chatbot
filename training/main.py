"""
ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ê°ì • ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸

ì‚¬ìš©ë²•:
    python main.py --mode train --batch_size 16 --epochs 10
    python main.py --mode evaluate --model_path checkpoints/best_model.pt
"""
import argparse
import os
import torch
from transformers import AutoTokenizer
import logging
import json
from datetime import datetime

from data_loader import load_emotion_data, create_data_loaders
from model import create_model
from train import Trainer
from visualize import plot_training_history

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    """ì»¤ë§¨ë“œ ë¼ì¸ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='ê°ì • ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ')
    
    # ëª¨ë“œ
    parser.add_argument('--mode', type=str, default='train',
                        choices=['train', 'evaluate', 'predict'],
                        help='ì‹¤í–‰ ëª¨ë“œ: train, evaluate, predict')
    
    # ë°ì´í„°
    parser.add_argument('--train_data', type=str,
                        default='data/processed/train.csv',
                        help='í•™ìŠµ ë°ì´í„° ê²½ë¡œ')
    parser.add_argument('--val_data', type=str,
                        default='data/processed/val.csv',
                        help='ê²€ì¦ ë°ì´í„° ê²½ë¡œ')
    parser.add_argument('--test_data', type=str,
                        default='data/processed/test.csv',
                        help='í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ')
    parser.add_argument('--text_column', type=str, default='text',
                        help='í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…')
    parser.add_argument('--label_column', type=str, default='label',
                        help='ë¼ë²¨ ì»¬ëŸ¼ëª…')
    
    # ëª¨ë¸
    parser.add_argument('--model_name', type=str,
                        default='klue/bert-base',
                        help='Hugging Face ëª¨ë¸ ì´ë¦„ (ì˜ˆ: klue/bert-base, skt/kobert-base-v1)')
    parser.add_argument('--num_labels', type=int, default=5,
                        help='ê°ì • í´ë˜ìŠ¤ ìˆ˜')
    parser.add_argument('--dropout_rate', type=float, default=0.3,
                        help='Dropout ë¹„ìœ¨')
    parser.add_argument('--freeze_bert', action='store_true',
                        help='BERT íŒŒë¼ë¯¸í„° ë™ê²° (ë¶„ë¥˜ í—¤ë“œë§Œ í•™ìŠµ)')
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument('--batch_size', type=int, default=16,
                        help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--epochs', type=int, default=10,
                        help='ì—í­ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=2e-5,
                        help='í•™ìŠµë¥ ')
    parser.add_argument('--max_length', type=int, default=128,
                        help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´')
    parser.add_argument('--warmup_steps', type=int, default=0,
                        help='Warmup ìŠ¤í… ìˆ˜')
    parser.add_argument('--max_grad_norm', type=float, default=1.0,
                        help='Gradient clipping ì„ê³„ê°’')
    parser.add_argument('--early_stopping_patience', type=int, default=3,
                        help='Early stopping ì¸ë‚´ì‹¬')
    
    # ì €ì¥/ë¡œë“œ
    parser.add_argument('--output_dir', type=str, default='checkpoints',
                        help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--model_path', type=str, default=None,
                        help='ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ (evaluate/predict ëª¨ë“œìš©)')
    parser.add_argument('--save_history', action='store_true',
                        help='í•™ìŠµ íˆìŠ¤í† ë¦¬ JSONìœ¼ë¡œ ì €ì¥')
    
    # ê¸°íƒ€
    parser.add_argument('--num_workers', type=int, default=0,
                        help='ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ (WindowsëŠ” 0 ê¶Œì¥)')
    parser.add_argument('--seed', type=int, default=42,
                        help='ëœë¤ ì‹œë“œ')
    
    return parser.parse_args()


def set_seed(seed: int):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    import random
    import numpy as np
    random.seed(seed)
    np.random.seed(seed)
    logger.info(f"ğŸŒ± ì‹œë“œ ì„¤ì •: {seed}")


def train_mode(args):
    """í•™ìŠµ ëª¨ë“œ"""
    logger.info("=" * 80)
    logger.info("ğŸš€ ê°ì • ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    logger.info("=" * 80)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        logger.info(f"   - GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"   - CUDA ë²„ì „: {torch.version.cuda}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # 1. ë°ì´í„° ë¡œë“œ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“‚ ë°ì´í„° ë¡œë“œ")
    logger.info("=" * 80)
    train_df, val_df, test_df = load_emotion_data(
        train_path=args.train_data,
        val_path=args.val_data,
        test_path=args.test_data if os.path.exists(args.test_data) else None,
        text_column=args.text_column,
        label_column=args.label_column
    )
    
    # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë“œ")
    logger.info("=" * 80)
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    logger.info(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {args.model_name}")
    
    # 3. DataLoader ìƒì„±
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ”„ DataLoader ìƒì„±")
    logger.info("=" * 80)
    train_loader, val_loader, test_loader = create_data_loaders(
        train_df=train_df,
        val_df=val_df,
        test_df=test_df,
        tokenizer=tokenizer,
        batch_size=args.batch_size,
        max_length=args.max_length,
        text_column=args.text_column,
        label_column=args.label_column,
        num_workers=args.num_workers
    )
    
    # 4. ëª¨ë¸ ìƒì„±
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¤– ëª¨ë¸ ìƒì„±")
    logger.info("=" * 80)
    model = create_model(
        model_name=args.model_name,
        num_labels=args.num_labels,
        dropout_rate=args.dropout_rate,
        freeze_bert=args.freeze_bert,
        device=device
    )
    
    # 5. Trainer ìƒì„±
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ‹ï¸ Trainer ìƒì„±")
    logger.info("=" * 80)
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        max_grad_norm=args.max_grad_norm
    )
    
    # 6. í•™ìŠµ ì‹¤í–‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = os.path.join(args.output_dir, f'best_model_{timestamp}.pt')
    
    history = trainer.train(
        num_epochs=args.epochs,
        save_path=save_path,
        early_stopping_patience=args.early_stopping_patience
    )
    
    # 7. í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥ (ì˜µì…˜)
    if args.save_history:
        history_path = os.path.join(args.output_dir, f'history_{timestamp}.json')
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥: {history_path}")
    
    # 8. ì‹œê°í™”
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ˆ í•™ìŠµ ê²°ê³¼ ì‹œê°í™”")
    logger.info("=" * 80)
    graph_path = os.path.join(args.output_dir, f'training_history_{timestamp}.png')
    plot_training_history(history, save_path=graph_path)
    
    logger.info("\n" + "=" * 80)
    logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
    logger.info(f"   - ëª¨ë¸ ì €ì¥: {save_path}")
    logger.info(f"   - ê·¸ë˜í”„ ì €ì¥: {graph_path}")
    logger.info("=" * 80)


def evaluate_mode(args):
    """í‰ê°€ ëª¨ë“œ"""
    logger.info("=" * 80)
    logger.info("ğŸ“Š ëª¨ë¸ í‰ê°€")
    logger.info("=" * 80)
    
    if not args.model_path or not os.path.exists(args.model_path):
        logger.error("âŒ --model_pathë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
        return
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}")
    
    # ë°ì´í„° ë¡œë“œ
    _, val_df, test_df = load_emotion_data(
        train_path=args.train_data,
        val_path=args.val_data,
        test_path=args.test_data if os.path.exists(args.test_data) else None,
        text_column=args.text_column,
        label_column=args.label_column
    )
    
    # í† í¬ë‚˜ì´ì €
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    
    # DataLoader
    _, val_loader, test_loader = create_data_loaders(
        train_df=val_df,  # ë”ë¯¸
        val_df=test_df if test_df is not None else val_df,
        test_df=None,
        tokenizer=tokenizer,
        batch_size=args.batch_size,
        max_length=args.max_length,
        text_column=args.text_column,
        label_column=args.label_column,
        num_workers=args.num_workers
    )
    
    # ëª¨ë¸ ë¡œë“œ
    model = create_model(
        model_name=args.model_name,
        num_labels=args.num_labels,
        dropout_rate=args.dropout_rate,
        freeze_bert=False,
        device=device
    )
    
    checkpoint = torch.load(args.model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ: {args.model_path}")
    
    # Trainerë¡œ í‰ê°€
    trainer = Trainer(
        model=model,
        train_loader=val_loader,  # ë”ë¯¸
        val_loader=val_loader,
        device=device
    )
    
    val_loss, val_acc, val_f1, report = trainer.validate()
    
    logger.info(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
    logger.info(f"   - Loss: {val_loss:.4f}")
    logger.info(f"   - Accuracy: {val_acc:.4f}")
    logger.info(f"   - F1 (weighted): {val_f1:.4f}")
    
    logger.info(f"\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
    for label_name in model.id2label.values():
        if label_name in report:
            metrics = report[label_name]
            logger.info(f"   - {label_name}: "
                      f"P={metrics['precision']:.3f}, "
                      f"R={metrics['recall']:.3f}, "
                      f"F1={metrics['f1-score']:.3f}, "
                      f"Support={metrics['support']}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_args()
    
    # ì‹œë“œ ê³ ì •
    set_seed(args.seed)
    
    # ì„¤ì • ì¶œë ¥
    logger.info("\n" + "=" * 80)
    logger.info("âš™ï¸ ì‹¤í–‰ ì„¤ì •")
    logger.info("=" * 80)
    for arg, value in vars(args).items():
        logger.info(f"   - {arg}: {value}")
    
    # ëª¨ë“œë³„ ì‹¤í–‰
    if args.mode == 'train':
        train_mode(args)
    elif args.mode == 'evaluate':
        evaluate_mode(args)
    elif args.mode == 'predict':
        logger.info("âŒ predict ëª¨ë“œëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        logger.error(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {args.mode}")


if __name__ == '__main__':
    main()
