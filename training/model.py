"""
λ¨λΈ μ •μ λ¨λ“
KLUE/KoBERT κΈ°λ° κ°μ • λ¶„λ¥ λ¨λΈ
"""
import torch
import torch.nn as nn
from transformers import AutoModel, AutoConfig
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EmotionClassifier(nn.Module):
    """
    KLUE/KoBERT κΈ°λ° κ°μ • λ¶„λ¥ λ¨λΈ
    """
    
    def __init__(
        self,
        model_name: str = 'klue/bert-base',
        num_labels: int = 5,
        dropout_rate: float = 0.3,
        freeze_bert: bool = False
    ):
        """
        Args:
            model_name: Hugging Face λ¨λΈ μ΄λ¦„
            num_labels: κ°μ • ν΄λμ¤ μ
            dropout_rate: Dropout λΉ„μ¨
            freeze_bert: BERT νλΌλ―Έν„° λ™κ²° μ—¬λ¶€
        """
        super(EmotionClassifier, self).__init__()
        
        self.model_name = model_name
        self.num_labels = num_labels
        
        # BERT λ¨λΈ λ΅λ“
        logger.info(f"π¤– λ¨λΈ λ΅λ“: {model_name}")
        self.bert = AutoModel.from_pretrained(model_name)
        self.config = AutoConfig.from_pretrained(model_name)
        
        # BERT νλΌλ―Έν„° λ™κ²° (μµμ…)
        if freeze_bert:
            logger.info("β„οΈ BERT νλΌλ―Έν„° λ™κ²°")
            for param in self.bert.parameters():
                param.requires_grad = False
        
        # λ¶„λ¥ ν—¤λ“
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(self.config.hidden_size, num_labels)
        
        # κ°μ • λΌλ²¨ λ§¤ν•‘ (κΈ°λ³Έκ°’)
        self.id2label = {
            0: "joy",
            1: "sad",
            2: "anxiety",
            3: "anger",
            4: "neutral"
        }
        self.label2id = {v: k for k, v in self.id2label.items()}
        
        logger.info(f"β… λ¨λΈ μ΄κΈ°ν™” μ™„λ£")
        logger.info(f"   - BERT hidden size: {self.config.hidden_size}")
        logger.info(f"   - ν΄λμ¤ μ: {num_labels}")
        logger.info(f"   - Dropout: {dropout_rate}")
    
    def forward(self, input_ids, attention_mask, labels=None):
        """
        Forward pass
        
        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]
            labels: [batch_size] (μµμ…)
        
        Returns:
            loss (λΌλ²¨μ΄ μλ” κ²½μ°), logits, hidden_states
        """
        # BERT μΈμ½”λ”©
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        # [CLS] ν† ν°μ hidden state μ‚¬μ©
        pooled_output = outputs.pooler_output  # [batch_size, hidden_size]
        
        # Dropout + λ¶„λ¥
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)  # [batch_size, num_labels]
        
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits, labels)
        
        return {
            'loss': loss,
            'logits': logits,
            'hidden_states': outputs.last_hidden_state
        }
    
    def predict(self, input_ids, attention_mask):
        """
        μμΈ΅ (ν‰κ°€ λ¨λ“)
        
        Returns:
            predicted_labels, probabilities
        """
        self.eval()
        with torch.no_grad():
            outputs = self.forward(input_ids, attention_mask)
            logits = outputs['logits']
            probabilities = torch.softmax(logits, dim=-1)
            predicted_labels = torch.argmax(probabilities, dim=-1)
        
        return predicted_labels, probabilities
    
    def get_trainable_parameters(self):
        """ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° μ λ°ν™"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_total_parameters(self):
        """μ „μ²΄ νλΌλ―Έν„° μ λ°ν™"""
        return sum(p.numel() for p in self.parameters())


def create_model(
    model_name: str = 'klue/bert-base',
    num_labels: int = 5,
    dropout_rate: float = 0.3,
    freeze_bert: bool = False,
    device: str = 'cuda'
) -> EmotionClassifier:
    """
    λ¨λΈ μƒμ„± λ° λ””λ°”μ΄μ¤ μ΄λ™
    
    Args:
        model_name: Hugging Face λ¨λΈ μ΄λ¦„
        num_labels: κ°μ • ν΄λμ¤ μ
        dropout_rate: Dropout λΉ„μ¨
        freeze_bert: BERT νλΌλ―Έν„° λ™κ²° μ—¬λ¶€
        device: 'cuda' λλ” 'cpu'
    
    Returns:
        λ¨λΈ κ°μ²΄
    """
    model = EmotionClassifier(
        model_name=model_name,
        num_labels=num_labels,
        dropout_rate=dropout_rate,
        freeze_bert=freeze_bert
    )
    
    # λ””λ°”μ΄μ¤ μ΄λ™
    model = model.to(device)
    
    # νλΌλ―Έν„° μ •λ³΄ μ¶λ ¥
    total_params = model.get_total_parameters()
    trainable_params = model.get_trainable_parameters()
    
    logger.info(f"π”Ά μ „μ²΄ νλΌλ―Έν„°: {total_params:,}")
    logger.info(f"π”Ά ν•™μµ κ°€λ¥ νλΌλ―Έν„°: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
    logger.info(f"π–¥οΈ λ””λ°”μ΄μ¤: {device}")
    
    return model
