"""
ìƒˆë¡œìš´ í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
- í•œêµ­ì–´_ë‹¨ë°œì„±_ëŒ€í™”_ë°ì´í„°ì…‹.xlsx
- í•œêµ­ì–´_ì—°ì†ì _ëŒ€í™”_ë°ì´í„°ì…‹.xlsx

ê¸°ì¡´ ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ì™€ í†µí•©í•˜ì—¬ í•™ìŠµìš© ë°ì´í„° ìƒì„±
"""
import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime

# ê°ì • ë¼ë²¨ ë§¤í•‘ (ê¸°ì¡´ ë°ì´í„°ì…‹ê³¼ í†µì¼)
EMOTION_MAP = {
    'í–‰ë³µ': 'joy',
    'ê¸°ì¨': 'joy',
    'ìŠ¬í””': 'sad',
    'ë¶ˆì•ˆ': 'anxiety',
    'ê³µí¬': 'anxiety',  # ê³µí¬ë¥¼ ë¶ˆì•ˆìœ¼ë¡œ ë§¤í•‘
    'ë‹¹í™©': 'anxiety',
    'ë¶„ë…¸': 'anger',
    'í™”ë‚¨': 'anger',
    'í˜ì˜¤': 'anger',  # í˜ì˜¤ë¥¼ ë¶„ë…¸ë¡œ ë§¤í•‘
    'ë†€ëŒ': 'neutral',  # ë†€ëŒì„ ì¤‘ë¦½ìœ¼ë¡œ ë§¤í•‘
    'ì¤‘ë¦½': 'neutral',
    'ìƒì²˜': 'sad',
}

# ìµœì¢… 5ê°œ ê°ì • í´ë˜ìŠ¤
EMOTION_CLASSES = ['joy', 'sad', 'anxiety', 'anger', 'neutral']
LABEL_TO_ID = {emotion: idx for idx, emotion in enumerate(EMOTION_CLASSES)}


def preprocess_single_conversation():
    """ë‹¨ë°œì„± ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"""
    print("\n" + "=" * 80)
    print("1ï¸âƒ£ í•œêµ­ì–´_ë‹¨ë°œì„±_ëŒ€í™”_ë°ì´í„°ì…‹.xlsx ì „ì²˜ë¦¬")
    print("=" * 80)
    
    df = pd.read_excel('raw/í•œêµ­ì–´_ë‹¨ë°œì„±_ëŒ€í™”_ë°ì´í„°ì…‹.xlsx')
    print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(df):,} samples")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (Sentence, Emotion)
    df = df[['Sentence', 'Emotion']].copy()
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    df = df.dropna()
    print(f"   - ê²°ì¸¡ì¹˜ ì œê±° í›„: {len(df):,} samples")
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    df = df[df['Sentence'].str.strip() != '']
    df = df[df['Emotion'].str.strip() != '']
    print(f"   - ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: {len(df):,} samples")
    
    # ê°ì • ë¼ë²¨ ë§¤í•‘
    df['emotion'] = df['Emotion'].map(EMOTION_MAP)
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ê°ì • í™•ì¸
    unmapped = df[df['emotion'].isna()]['Emotion'].unique()
    if len(unmapped) > 0:
        print(f"   âš ï¸ ë§¤í•‘ë˜ì§€ ì•Šì€ ê°ì •: {unmapped}")
        df = df.dropna(subset=['emotion'])
    
    print(f"   - ê°ì • ë§¤í•‘ í›„: {len(df):,} samples")
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    df = df.rename(columns={'Sentence': 'text'})
    df = df[['text', 'emotion']].copy()
    
    # label_id ì¶”ê°€
    df['label_id'] = df['emotion'].map(LABEL_TO_ID)
    
    # ê°ì • ë¶„í¬ ì¶œë ¥
    print(f"\nğŸ“Š ê°ì • ë¶„í¬:")
    for emotion in EMOTION_CLASSES:
        count = (df['emotion'] == emotion).sum()
        percentage = count / len(df) * 100
        print(f"   - {emotion}: {count:,} ({percentage:.1f}%)")
    
    return df


def preprocess_continuous_conversation():
    """ì—°ì†ì  ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"""
    print("\n" + "=" * 80)
    print("2ï¸âƒ£ í•œêµ­ì–´_ì—°ì†ì _ëŒ€í™”_ë°ì´í„°ì…‹.xlsx ì „ì²˜ë¦¬")
    print("=" * 80)
    
    df = pd.read_excel('raw/í•œêµ­ì–´_ì—°ì†ì _ëŒ€í™”_ë°ì´í„°ì…‹.xlsx')
    print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ: {len(df):,} samples")
    
    # ì²« ë²ˆì§¸ í—¤ë” í–‰ ì œê±° (dialog #, ë°œí™”, ê°ì • í–‰)
    df = df[df['Unnamed: 0'] != 'dialog #'].copy()
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ë°œí™”, ê°ì •)
    # Unnamed: 1 = ë°œí™”, Unnamed: 2 = ê°ì •
    df = df[['Unnamed: 1', 'Unnamed: 2']].copy()
    df.columns = ['text', 'emotion_raw']
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    df = df.dropna()
    print(f"   - ê²°ì¸¡ì¹˜ ì œê±° í›„: {len(df):,} samples")
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    df = df[df['text'].str.strip() != '']
    df = df[df['emotion_raw'].str.strip() != '']
    print(f"   - ë¹ˆ ë¬¸ìì—´ ì œê±° í›„: {len(df):,} samples")
    
    # ì˜¤íƒ€ ì œê±° (ã…, ë¶„, ã…ˆì¤‘ë¦½, ë¶„ã„´, ì¤‘ë¦¼, ã„´ì¤‘ë¦½, ì¤„ ë“±)
    valid_emotions = set(EMOTION_MAP.keys())
    df = df[df['emotion_raw'].isin(valid_emotions)]
    print(f"   - ìœ íš¨í•œ ê°ì •ë§Œ ì„ íƒ í›„: {len(df):,} samples")
    
    # ê°ì • ë¼ë²¨ ë§¤í•‘
    df['emotion'] = df['emotion_raw'].map(EMOTION_MAP)
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ê°ì • í™•ì¸
    unmapped = df[df['emotion'].isna()]['emotion_raw'].unique()
    if len(unmapped) > 0:
        print(f"   âš ï¸ ë§¤í•‘ë˜ì§€ ì•Šì€ ê°ì •: {unmapped}")
        df = df.dropna(subset=['emotion'])
    
    print(f"   - ê°ì • ë§¤í•‘ í›„: {len(df):,} samples")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    df = df[['text', 'emotion']].copy()
    
    # label_id ì¶”ê°€
    df['label_id'] = df['emotion'].map(LABEL_TO_ID)
    
    # ê°ì • ë¶„í¬ ì¶œë ¥
    print(f"\nğŸ“Š ê°ì • ë¶„í¬:")
    for emotion in EMOTION_CLASSES:
        count = (df['emotion'] == emotion).sum()
        percentage = count / len(df) * 100
        print(f"   - {emotion}: {count:,} ({percentage:.1f}%)")
    
    return df


def load_existing_data():
    """ê¸°ì¡´ ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ ë°ì´í„° ë¡œë“œ"""
    print("\n" + "=" * 80)
    print("3ï¸âƒ£ ê¸°ì¡´ ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ ë°ì´í„° ë¡œë“œ")
    print("=" * 80)
    
    processed_path = Path('processed/emotion_corpus_full.csv')
    
    if processed_path.exists():
        df = pd.read_csv(processed_path)
        print(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(df):,} samples")
        
        # ê°ì • ë¶„í¬ ì¶œë ¥
        print(f"\nğŸ“Š ê°ì • ë¶„í¬:")
        for emotion in EMOTION_CLASSES:
            count = (df['emotion'] == emotion).sum()
            percentage = count / len(df) * 100
            print(f"   - {emotion}: {count:,} ({percentage:.1f}%)")
        
        return df
    else:
        print("âš ï¸ ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None


def merge_and_save(df_existing, df_single, df_continuous):
    """ë°ì´í„° í†µí•© ë° ì €ì¥"""
    print("\n" + "=" * 80)
    print("4ï¸âƒ£ ë°ì´í„° í†µí•© ë° ì €ì¥")
    print("=" * 80)
    
    # ë°ì´í„° í†µí•©
    dfs = []
    sources = []
    
    if df_existing is not None:
        dfs.append(df_existing)
        sources.append('emotion_corpus')
        print(f"   - ê¸°ì¡´ ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜: {len(df_existing):,} samples")
    
    if df_single is not None:
        dfs.append(df_single)
        sources.append('single_conversation')
        print(f"   - ë‹¨ë°œì„± ëŒ€í™”: {len(df_single):,} samples")
    
    if df_continuous is not None:
        dfs.append(df_continuous)
        sources.append('continuous_conversation')
        print(f"   - ì—°ì†ì  ëŒ€í™”: {len(df_continuous):,} samples")
    
    df_merged = pd.concat(dfs, ignore_index=True)
    print(f"\nâœ… í†µí•© ì™„ë£Œ: {len(df_merged):,} samples")
    
    # ì¤‘ë³µ ì œê±° (text ê¸°ì¤€)
    original_len = len(df_merged)
    df_merged = df_merged.drop_duplicates(subset=['text'], keep='first')
    duplicates = original_len - len(df_merged)
    if duplicates > 0:
        print(f"   - ì¤‘ë³µ ì œê±°: {duplicates:,} samples")
        print(f"   - ìµœì¢…: {len(df_merged):,} samples")
    
    # ìµœì¢… ê°ì • ë¶„í¬
    print(f"\nğŸ“Š ìµœì¢… ê°ì • ë¶„í¬:")
    for emotion in EMOTION_CLASSES:
        count = (df_merged['emotion'] == emotion).sum()
        percentage = count / len(df_merged) * 100
        print(f"   - {emotion}: {count:,} ({percentage:.1f}%)")
    
    # ì €ì¥
    output_path = Path('processed/emotion_corpus_merged.csv')
    df_merged.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_samples': len(df_merged),
        'sources': sources,
        'emotion_distribution': {
            emotion: {
                'count': int((df_merged['emotion'] == emotion).sum()),
                'percentage': float((df_merged['emotion'] == emotion).sum() / len(df_merged) * 100)
            }
            for emotion in EMOTION_CLASSES
        },
        'emotion_classes': EMOTION_CLASSES,
        'label_mapping': LABEL_TO_ID,
        'duplicates_removed': duplicates
    }
    
    metadata_path = Path('processed/emotion_corpus_merged_metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    return df_merged


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "=" * 80)
    print("ğŸš€ ìƒˆë¡œìš´ í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    
    # 1. ë‹¨ë°œì„± ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
    df_single = preprocess_single_conversation()
    
    # 2. ì—°ì†ì  ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
    df_continuous = preprocess_continuous_conversation()
    
    # 3. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    df_existing = load_existing_data()
    
    # 4. í†µí•© ë° ì €ì¥
    df_merged = merge_and_save(df_existing, df_single, df_continuous)
    
    print("\n" + "=" * 80)
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   - ì´ ìƒ˜í”Œ ìˆ˜: {len(df_merged):,}")
    print(f"   - ê°ì • í´ë˜ìŠ¤: {len(EMOTION_CLASSES)}ê°œ")
    print(f"   - ì €ì¥ ê²½ë¡œ: processed/emotion_corpus_merged.csv")
    print(f"\nğŸ’¡ í•™ìŠµ ì‹œ ì‚¬ìš©:")
    print(f"   python training/train_krbert_hf.py --data_path data/processed/emotion_corpus_merged.csv")


if __name__ == '__main__':
    main()
