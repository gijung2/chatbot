"""
ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
Training.json + Validation.json â†’ K-Foldìš© ì „ì²´ ë°ì´í„°ì…‹ ìƒì„±
"""
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict
import re

# ê°ì • ì½”ë“œ ë§¤í•‘ (14ê°œ â†’ 5ê°œ í´ë˜ìŠ¤)
EMOTION_MAPPING = {
    # anger (ë¶„ë…¸)
    'E10': 'anger',  # ë¶„ë…¸
    'E18': 'anger',  # ì§œì¦
    'E19': 'anger',  # íˆ´íˆ´ê±°ë¦¼
    
    # sad (ìŠ¬í””)
    'E22': 'sad',    # ìŠ¬í””
    'E40': 'sad',    # ì‹¤ë§
    'E49': 'sad',    # ì–µìš¸í•¨
    'E56': 'sad',    # ê´´ë¡œì›€
    
    # anxiety (ë¶ˆì•ˆ)
    'E25': 'anxiety',  # ë‹¹í™©
    'E30': 'anxiety',  # ë‘ë ¤ì›€
    'E31': 'anxiety',  # ê¸´ì¥
    'E35': 'anxiety',  # ê±±ì •
    'E37': 'anxiety',  # ì•ˆì ˆë¶€ì ˆëª»í•¨
    'E50': 'anxiety',  # ì´ˆì¡°
    
    # joy (ê¸°ì¨)
    'E64': 'joy',     # ê¸°ì¨
    
    # neutral (ì¤‘ë¦½)
    'E66': 'neutral'  # í¸ì•ˆ
}

# ë¼ë²¨ ë§¤í•‘
LABEL2ID = {'joy': 0, 'sad': 1, 'anxiety': 2, 'anger': 3, 'neutral': 4}
ID2LABEL = {v: k for k, v in LABEL2ID.items()}


def load_json_data(file_path: str) -> List[Dict]:
    """JSON íŒŒì¼ ë¡œë“œ"""
    print(f"ğŸ“‚ ë¡œë”©: {file_path}")
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"   âœ… {len(data):,}ê°œ ëŒ€í™” ë¡œë“œ")
    return data


def extract_conversations(data: List[Dict]) -> List[Dict]:
    """ëŒ€í™” ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    conversations = []
    
    for item in data:
        try:
            # ê°ì • íƒ€ì… ì¶”ì¶œ
            emotion_type = item['profile']['emotion']['type']
            
            # ë§¤í•‘ë˜ì§€ ì•Šì€ ê°ì •ì€ ì œì™¸
            if emotion_type not in EMOTION_MAPPING:
                continue
            
            emotion_label = EMOTION_MAPPING[emotion_type]
            
            # ëŒ€í™” ë‚´ìš© ì¶”ì¶œ (HS01, HS02, HS03 - ì‚¬ìš©ì ë°œí™”)
            content = item['talk']['content']
            
            # ê° ì‚¬ìš©ì ë°œí™”ë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ
            for key in ['HS01', 'HS02', 'HS03']:
                if key in content:
                    text = content[key].strip()
                    
                    # ë¹ˆ í…ìŠ¤íŠ¸ ì œì™¸
                    if not text:
                        continue
                    
                    conversations.append({
                        'text': text,
                        'emotion': emotion_label,
                        'label_id': LABEL2ID[emotion_label],
                        'emotion_code': emotion_type
                    })
        
        except (KeyError, TypeError) as e:
            continue
    
    return conversations


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def preprocess_corpus():
    """ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    print("=" * 80)
    print("ğŸ“Š ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ ì „ì²˜ë¦¬ ì‹œì‘")
    print("=" * 80)
    
    # ê²½ë¡œ ì„¤ì •
    raw_dir = Path(__file__).parent / 'raw'
    processed_dir = Path(__file__).parent / 'processed'
    processed_dir.mkdir(exist_ok=True)
    
    training_file = raw_dir / 'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json'
    validation_file = raw_dir / 'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.json'
    
    # 1. Training.json ë¡œë“œ ë° ì¶”ì¶œ
    print("\n" + "=" * 80)
    print("ğŸ“ Training ë°ì´í„° ì²˜ë¦¬")
    print("=" * 80)
    training_data = load_json_data(str(training_file))
    training_conversations = extract_conversations(training_data)
    print(f"   âœ… {len(training_conversations):,}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
    
    # 2. Validation.json ë¡œë“œ ë° ì¶”ì¶œ
    print("\n" + "=" * 80)
    print("ğŸ“ Validation ë°ì´í„° ì²˜ë¦¬")
    print("=" * 80)
    validation_data = load_json_data(str(validation_file))
    validation_conversations = extract_conversations(validation_data)
    print(f"   âœ… {len(validation_conversations):,}ê°œ ìƒ˜í”Œ ì¶”ì¶œ")
    
    # 3. ë°ì´í„° í•©ì¹˜ê¸°
    print("\n" + "=" * 80)
    print("ğŸ”— ë°ì´í„° í†µí•©")
    print("=" * 80)
    all_conversations = training_conversations + validation_conversations
    print(f"   âœ… ì´ {len(all_conversations):,}ê°œ ìƒ˜í”Œ")
    
    # 4. DataFrame ìƒì„±
    df = pd.DataFrame(all_conversations)
    
    # í…ìŠ¤íŠ¸ ì •ì œ
    print("\nğŸ§¹ í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
    df['text'] = df['text'].apply(clean_text)
    
    # ì¤‘ë³µ ì œê±°
    before_count = len(df)
    df = df.drop_duplicates(subset=['text'], keep='first')
    after_count = len(df)
    print(f"   - ì¤‘ë³µ ì œê±°: {before_count:,} â†’ {after_count:,} ({before_count - after_count:,}ê°œ ì œê±°)")
    
    # 5. í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print("\n" + "=" * 80)
    print("ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬")
    print("=" * 80)
    print(f"{'í´ë˜ìŠ¤':<15} {'ìƒ˜í”Œ ìˆ˜':>10} {'ë¹„ìœ¨':>10}")
    print("-" * 40)
    for label in sorted(LABEL2ID.keys()):
        count = (df['emotion'] == label).sum()
        percentage = count / len(df) * 100
        print(f"{label:<15} {count:>10,} {percentage:>9.1f}%")
    print("-" * 40)
    print(f"{'ì „ì²´':<15} {len(df):>10,} {100.0:>9.1f}%")
    
    # 6. ì €ì¥
    output_file = processed_dir / 'emotion_corpus_full.csv'
    df.to_csv(output_file, index=False, encoding='utf-8')
    
    print("\n" + "=" * 80)
    print("ğŸ’¾ ì €ì¥ ì™„ë£Œ")
    print("=" * 80)
    print(f"   - íŒŒì¼: {output_file}")
    print(f"   - í¬ê¸°: {output_file.stat().st_size / (1024*1024):.2f} MB")
    print(f"   - ìƒ˜í”Œ ìˆ˜: {len(df):,}")
    print(f"   - ì»¬ëŸ¼: {list(df.columns)}")
    
    # 7. ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'total_samples': len(df),
        'num_classes': len(LABEL2ID),
        'class_distribution': df['emotion'].value_counts().to_dict(),
        'label2id': LABEL2ID,
        'id2label': ID2LABEL,
        'emotion_mapping': EMOTION_MAPPING,
        'columns': list(df.columns),
        'source_files': [
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json',
            'ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Validation.json'
        ]
    }
    
    metadata_file = processed_dir / 'emotion_corpus_metadata.json'
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"   - ë©”íƒ€ë°ì´í„°: {metadata_file}")
    
    print("\n" + "=" * 80)
    print("âœ… ì „ì²˜ë¦¬ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nğŸ’¡ K-Fold í•™ìŠµ ì‹¤í–‰:")
    print(f"   python training/main_kfold.py --data_path {output_file} --k_folds 5 --epochs 10")
    
    return df


if __name__ == '__main__':
    preprocess_corpus()
