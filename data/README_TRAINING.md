# ğŸ“Š ë°ì´í„° ì „ì²˜ë¦¬ ë° í•™ìŠµ ê°€ì´ë“œ

## ğŸ—‚ï¸ ë°ì´í„°ì…‹ êµ¬ì¡°

### í†µí•© ë°ì´í„°ì…‹ (emotion_corpus_merged.csv) - **ê¶Œì¥**
- **ì´ ìƒ˜í”Œ**: 131,091ê°œ
- **ì¶œì²˜**: 3ê°œ ë°ì´í„°ì…‹ í†µí•©
  1. ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜ (AI Hub): 41,387 samples
  2. í•œêµ­ì–´_ë‹¨ë°œì„±_ëŒ€í™”_ë°ì´í„°ì…‹: 38,594 samples
  3. í•œêµ­ì–´_ì—°ì†ì _ëŒ€í™”_ë°ì´í„°ì…‹: 55,600 samples

### ê°ì • ë¶„í¬
| ê°ì • | ê°œìˆ˜ | ë¹„ìœ¨ |
|------|------|------|
| joy (ê¸°ì¨) | 9,037 | 6.9% |
| sad (ìŠ¬í””) | 18,074 | 13.8% |
| anxiety (ë¶ˆì•ˆ) | 23,090 | 17.6% |
| anger (ë¶„ë…¸) | 23,854 | 18.2% |
| neutral (ì¤‘ë¦½) | 57,036 | 43.5% |

---

## ğŸš€ 1ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬

### ê¸°ì¡´ ë°ì´í„°ë§Œ ì‚¬ìš© (41K samples)
```bash
cd data
python preprocess_emotion_corpus.py
```
ì¶œë ¥: `processed/emotion_corpus_full.csv`

### ìƒˆ ë°ì´í„° ì¶”ê°€ + í†µí•© (131K samples) â­ **ê¶Œì¥**
```bash
cd data
python preprocess_new_datasets.py
```
ì¶œë ¥: `processed/emotion_corpus_merged.csv`

---

## ğŸ“ 2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ

### ì˜µì…˜ A: Google Colab (GPU ë¬´ë£Œ, 2-3ì‹œê°„) â­ **ê°€ì¥ ë¹ ë¦„**

1. **Colab ì ‘ì†**: https://colab.research.google.com/
2. **ë…¸íŠ¸ë¶ ì—…ë¡œë“œ**: `colab_training.ipynb`
3. **GPU í™œì„±í™”**: ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > GPU
4. **ë°ì´í„° ì—…ë¡œë“œ**: `emotion_corpus_merged.csv` (ë˜ëŠ” `emotion_corpus_full.csv`)
5. **ì‹¤í–‰**: ëŸ°íƒ€ì„ > ëª¨ë‘ ì‹¤í–‰

**ì˜ˆìƒ ì‹œê°„**: 2-3ì‹œê°„ (GPU T4)  
**ì˜ˆìƒ ì„±ëŠ¥**: Accuracy 87-92%, F1 0.86-0.91

---

### ì˜µì…˜ B: ë¡œì»¬ CPU í•™ìŠµ (6-10ì‹œê°„)

#### KR-BERT í•™ìŠµ (Hugging Face Trainer)
```bash
# í†µí•© ë°ì´í„° (131K samples, ê¶Œì¥)
python training/train_krbert_hf.py \
    --data_path data/processed/emotion_corpus_merged.csv \
    --model_name snunlp/KR-Medium \
    --epochs 12 \
    --batch_size 32 \
    --k_folds 2

# ê¸°ì¡´ ë°ì´í„°ë§Œ (41K samples)
python training/train_krbert_hf.py \
    --data_path data/processed/emotion_corpus_full.csv \
    --model_name snunlp/KR-Medium \
    --epochs 12 \
    --batch_size 32 \
    --k_folds 2
```

#### KLUE BERT í•™ìŠµ (ì»¤ìŠ¤í…€ Trainer)
```bash
python training/main_kfold.py \
    --data_path data/processed/emotion_corpus_merged.csv \
    --model_name klue/bert-base \
    --epochs 12 \
    --batch_size 16 \
    --k_folds 2
```

**ì˜ˆìƒ ì‹œê°„**: 
- í†µí•© ë°ì´í„° (131K): 10-12ì‹œê°„
- ê¸°ì¡´ ë°ì´í„° (41K): 6-8ì‹œê°„

---

## ğŸ“¦ 3ë‹¨ê³„: ëª¨ë¸ ì €ì¥ ìœ„ì¹˜

### Colab í•™ìŠµ í›„
```
ë‹¤ìš´ë¡œë“œ íŒŒì¼:
â”œâ”€â”€ best_model_fold1_20251102_XXXXXX.zip  (ëª¨ë¸ ì••ì¶• íŒŒì¼)
â””â”€â”€ kfold_summary.json                     (í•™ìŠµ ê²°ê³¼ ìš”ì•½)
```

**ì••ì¶• í•´ì œ ìœ„ì¹˜**: `checkpoints_kfold/fold1_best_model_20251102_XXXXXX/`

### ë¡œì»¬ í•™ìŠµ í›„
```
checkpoints_krbert/
â”œâ”€â”€ fold1_best_model_20251102_XXXXXX/
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ vocab.txt
â”œâ”€â”€ fold2_best_model_20251102_XXXXXX/
â””â”€â”€ kfold_summary_20251102_XXXXXX.json
```

---

## ğŸ§ª 4ë‹¨ê³„: ëª¨ë¸ í…ŒìŠ¤íŠ¸

### ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# ëª¨ë¸ ë¡œë“œ
model_path = "checkpoints_krbert/fold1_best_model_20251102_XXXXXX"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# ê°ì • ë§¤í•‘
emotion_labels = ['joy', 'sad', 'anxiety', 'anger', 'neutral']

# ì¶”ë¡ 
def predict_emotion(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
    
    emotion = emotion_labels[predicted_class]
    confidence = torch.softmax(logits, dim=1)[0][predicted_class].item()
    
    return emotion, confidence

# í…ŒìŠ¤íŠ¸
text = "ì˜¤ëŠ˜ ì •ë§ ê¸°ë¶„ì´ ì¢‹ì•„ìš”!"
emotion, confidence = predict_emotion(text)
print(f"ì…ë ¥: {text}")
print(f"ì˜ˆì¸¡ ê°ì •: {emotion} (ì‹ ë¢°ë„: {confidence:.2%})")
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| ë°ì´í„°ì…‹ | ìƒ˜í”Œ ìˆ˜ | ê· í˜•ë„ | ì˜ˆìƒ Accuracy | ì˜ˆìƒ F1 | í•™ìŠµ ì‹œê°„ (Colab) |
|---------|---------|--------|--------------|---------|------------------|
| ê¸°ì¡´ (emotion_corpus_full) | 41,387 | ë¶ˆê· í˜• | 85-90% | 0.83-0.88 | 1-2ì‹œê°„ |
| **í†µí•© (emotion_corpus_merged)** | **131,091** | **ê· í˜• ê°œì„ ** | **87-92%** | **0.86-0.91** | **2-3ì‹œê°„** |

---

## ğŸ’¡ ì¶”ê°€ ê°œì„  ë°©ì•ˆ

### 1. ë” í° ëª¨ë¸ ì‚¬ìš©
```bash
python training/train_krbert_hf.py \
    --model_name klue/roberta-large \
    --batch_size 16 \
    --epochs 15
```

### 2. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì¡°ì •
ì†Œìˆ˜ í´ë˜ìŠ¤(joy)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ

### 3. ë°ì´í„° ì¦ê°•
- ì—­ë²ˆì—­ (Back-translation)
- ë™ì˜ì–´ ì¹˜í™˜ (Synonym replacement)

### 4. ì•™ìƒë¸” ëª¨ë¸
- KR-BERT + KLUE BERT + RoBERTa ì•™ìƒë¸”

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### RTX 5070 GPU ì‚¬ìš© ë¶ˆê°€
**ì›ì¸**: SM 12.0 (Compute Capability 12.0) ë¯¸ì§€ì›  
**í•´ê²°**: Google Colab ë˜ëŠ” CPU í•™ìŠµ ì‚¬ìš©

### Out of Memory (OOM)
**í•´ê²°**: 
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
--batch_size 16  # ê¸°ë³¸ 32ì—ì„œ 16ìœ¼ë¡œ

# Gradient Accumulation ì‚¬ìš©
--gradient_accumulation_steps 2
```

### ë°ì´í„° íŒŒì¼ ì—†ìŒ
```bash
# ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰
cd data
python preprocess_new_datasets.py
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- **KR-BERT**: https://github.com/snunlp/KR-BERT
- **KLUE**: https://github.com/KLUE-benchmark/KLUE
- **Hugging Face Transformers**: https://huggingface.co/docs/transformers/
- **Google Colab**: https://colab.research.google.com/

---

## ğŸ“ ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:
1. Python 3.11 ì‚¬ìš© ì¤‘ì¸ì§€
2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª¨ë‘ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ (`pip install -r requirements.txt`)
3. ë°ì´í„° íŒŒì¼ì´ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ìˆëŠ”ì§€
4. GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œì§€ (Colab ì‚¬ìš© ê¶Œì¥)
